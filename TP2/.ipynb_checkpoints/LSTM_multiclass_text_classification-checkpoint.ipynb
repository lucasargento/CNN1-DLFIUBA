{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (4.50.2)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[K     |████████████████████████████████| 461 kB 66.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 51.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy) (49.6.0.post20201009)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 75.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 91.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (817 kB)\n",
      "\u001b[K     |████████████████████████████████| 817 kB 63.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 9.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 53.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Installing collected packages: smart-open, typer, pathy, cymem, wasabi, catalogue, srsly, pydantic, spacy-loggers, spacy-legacy, murmurhash, preshed, langcodes, blis, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.8 catalogue-2.0.8 confection-0.0.3 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.8 pathy-0.6.2 preshed-3.0.7 pydantic-1.9.2 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.3 typer-0.4.2 wasabi-0.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import jovian\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM in Pytorch with random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "x = torch.tensor([[1,2, 12,34, 56,78, 90,80],\n",
    "                 [12,45, 99,67, 6,23, 77,82],\n",
    "                 [3,24, 6,99, 12,56, 21,22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using two different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = nn.Embedding(100, 7, padding_idx=0)\n",
    "model2 = nn.LSTM(input_size=7, hidden_size=3, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model1(x)\n",
    "out2 = model2(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 7])\n",
      "tensor([[[ 1.6159, -1.1283, -0.8631,  1.5909, -0.1171, -1.4717,  1.4809],\n",
      "         [ 0.5187,  0.2979,  0.2123, -0.7887,  0.7360, -1.4162,  0.9776],\n",
      "         [ 0.1944,  0.5275, -2.0730,  0.5661, -1.0202, -0.2667,  0.9350],\n",
      "         [-1.0823,  0.8391,  0.9950, -0.7712,  0.2877, -0.3298, -0.6360],\n",
      "         [ 0.8716,  0.8908,  1.1256, -0.2872, -0.3344, -0.1445,  0.7279],\n",
      "         [ 0.3485, -0.3984, -0.1507, -0.3501, -0.1223, -0.4639, -0.3473],\n",
      "         [-1.0121,  1.7877, -1.7242, -0.4620, -1.1344, -0.4563, -0.0945],\n",
      "         [-0.9924, -0.8139, -0.7243,  0.4677, -0.9332, -0.0714,  0.7314]],\n",
      "\n",
      "        [[ 0.1944,  0.5275, -2.0730,  0.5661, -1.0202, -0.2667,  0.9350],\n",
      "         [-0.1743,  1.1546, -1.3543,  0.3184,  0.9935,  0.0681, -0.2036],\n",
      "         [ 1.4183, -1.5499,  1.2354,  0.3930,  0.5845, -1.2884,  0.9210],\n",
      "         [-0.1669,  1.9245, -0.3736,  1.4338,  0.6350,  0.9702, -0.3813],\n",
      "         [ 0.5783, -0.2574,  0.2171, -0.7534,  1.1099, -1.4365, -0.4764],\n",
      "         [ 0.3914,  1.5622,  0.3975,  1.4436, -0.1643, -1.6261,  0.2272],\n",
      "         [-0.2003, -0.8138, -0.6384, -0.1845, -0.9675,  0.4240, -0.0450],\n",
      "         [-0.2117,  0.6101, -1.0702, -2.1371,  0.7181,  0.4036, -0.0952]],\n",
      "\n",
      "        [[-1.5509, -0.0028,  0.5893, -0.0729,  0.7860, -0.7386,  0.8517],\n",
      "         [ 1.2820,  0.7028,  0.9121,  0.1291,  1.1382, -0.9323,  0.1718],\n",
      "         [ 0.5783, -0.2574,  0.2171, -0.7534,  1.1099, -1.4365, -0.4764],\n",
      "         [ 1.4183, -1.5499,  1.2354,  0.3930,  0.5845, -1.2884,  0.9210],\n",
      "         [ 0.1944,  0.5275, -2.0730,  0.5661, -1.0202, -0.2667,  0.9350],\n",
      "         [ 0.8716,  0.8908,  1.1256, -0.2872, -0.3344, -0.1445,  0.7279],\n",
      "         [-0.1494,  1.9469,  0.0450, -0.3436,  1.0553,  1.6641,  0.6356],\n",
      "         [ 0.1174, -0.4032, -0.1597, -1.0076, -1.2297, -0.0127,  0.6315]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out1.shape)\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2167,  0.6432,  0.0521],\n",
      "         [ 0.2513,  0.3342, -0.0186],\n",
      "         [ 0.4287,  0.0559, -0.0224]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "out, (ht, ct) = model2(out1)\n",
    "print(ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using nn.sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = nn.Sequential(nn.Embedding(100, 7, padding_idx=0),\n",
    "                        nn.LSTM(input_size=7, hidden_size=3, num_layers=1, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0809, -0.1606, -0.0996],\n",
      "         [-0.1139, -0.3909, -0.0904],\n",
      "         [-0.2203, -0.1624,  0.3150],\n",
      "         [-0.1882, -0.4972, -0.0665],\n",
      "         [ 0.0258,  0.1685, -0.0335],\n",
      "         [-0.0427, -0.0109, -0.0655],\n",
      "         [-0.1144, -0.0635,  0.0510],\n",
      "         [-0.3102, -0.1192,  0.2200]],\n",
      "\n",
      "        [[-0.2008, -0.0701,  0.3294],\n",
      "         [-0.0054, -0.2575,  0.0536],\n",
      "         [ 0.0018, -0.1735, -0.0111],\n",
      "         [ 0.2507, -0.2012,  0.2531],\n",
      "         [-0.0190, -0.0904,  0.0538],\n",
      "         [-0.1168, -0.2633,  0.0463],\n",
      "         [-0.0011, -0.0607, -0.0188],\n",
      "         [ 0.1373, -0.0878,  0.2853]],\n",
      "\n",
      "        [[ 0.1303,  0.4933, -0.0257],\n",
      "         [ 0.1684,  0.1061, -0.1077],\n",
      "         [-0.0374, -0.0190,  0.0270],\n",
      "         [-0.1203, -0.1521, -0.0141],\n",
      "         [-0.2221, -0.1144,  0.3227],\n",
      "         [-0.0637,  0.3279,  0.0285],\n",
      "         [-0.3127,  0.1586,  0.2328],\n",
      "         [-0.1614,  0.1067,  0.2020]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out, (ht, ct) = model3(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Text Classification\n",
    "\n",
    "We are going to predict item ratings based on customer reviews bsed on this dataset from Kaggle:\n",
    "https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6f6fb1132ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reviews.csv'"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "reviews = pd.read_csv(\"reviews.csv\")\n",
    "print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90a519424dc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review Text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
     ]
    }
   ],
   "source": [
    "reviews['Title'] = reviews['Title'].fillna('')\n",
    "reviews['Review Text'] = reviews['Review Text'].fillna('')\n",
    "reviews['review'] = reviews['Title'] + ' ' + reviews['Review Text']\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absolutely wonderful - silky and sexy and com...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happen...</td>\n",
       "      <td>5</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some major design flaws I had such high hopes ...</td>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My favorite buy! I love, love, love this jumps...</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flattering shirt This shirt is very flattering...</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating  review_length\n",
       "0   Absolutely wonderful - silky and sexy and com...       4              8\n",
       "1   Love this dress!  it's sooo pretty.  i happen...       5             62\n",
       "2  Some major design flaws I had such high hopes ...       3            102\n",
       "3  My favorite buy! I love, love, love this jumps...       5             25\n",
       "4  Flattering shirt This shirt is very flattering...       5             38"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keeping only relevant columns and calculating sentence lengths\n",
    "reviews = reviews[['review', 'Rating']]\n",
    "reviews.columns = ['review', 'rating']\n",
    "reviews['review_length'] = reviews['review'].apply(lambda x: len(x.split()))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing ratings to 0-numbering\n",
    "zero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\n",
    "reviews['rating'] = reviews['rating'].apply(lambda x: zero_numbering[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.832921740611425"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean sentence length\n",
    "np.mean(reviews['review_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tok = spacy.load('en')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in reviews.iterrows():\n",
    "    counts.update(tokenize(row['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 14138\n",
      "num_words after: 8263\n"
     ]
    }
   ],
   "source": [
    "#deleting infrequent words\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_length</th>\n",
       "      <th>encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absolutely wonderful - silky and sexy and com...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 8, 7, 9, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happen...</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>[[2, 10, 11, 12, 5, 13, 14, 15, 16, 5, 17, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some major design flaws I had such high hopes ...</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>[[54, 55, 56, 57, 17, 58, 59, 60, 61, 62, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My favorite buy! I love, love, love this jumps...</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>[[68, 109, 110, 2, 17, 10, 2, 10, 2, 10, 11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flattering shirt This shirt is very flattering...</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>[[122, 123, 11, 123, 52, 92, 122, 19, 124, 125...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating  review_length  \\\n",
       "0   Absolutely wonderful - silky and sexy and com...       3              8   \n",
       "1   Love this dress!  it's sooo pretty.  i happen...       4             62   \n",
       "2  Some major design flaws I had such high hopes ...       2            102   \n",
       "3  My favorite buy! I love, love, love this jumps...       4             25   \n",
       "4  Flattering shirt This shirt is very flattering...       4             38   \n",
       "\n",
       "                                             encoded  \n",
       "0  [[2, 3, 4, 5, 6, 7, 8, 7, 9, 0, 0, 0, 0, 0, 0,...  \n",
       "1  [[2, 10, 11, 12, 5, 13, 14, 15, 16, 5, 17, 18,...  \n",
       "2  [[54, 55, 56, 57, 17, 58, 59, 60, 61, 62, 11, ...  \n",
       "3  [[68, 109, 110, 2, 17, 10, 2, 10, 2, 10, 11, 1...  \n",
       "4  [[122, 123, 11, 123, 52, 92, 122, 19, 124, 125...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['encoded'] = reviews['review'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 5077, 4: 13131, 2: 2871, 1: 1565, 0: 842})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how balanced the dataset is\n",
    "Counter(reviews['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(reviews['encoded'])\n",
    "y = list(reviews['rating'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with fixed length input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fixed =  LSTM_fixed_len(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.239, val loss 1.218, val accuracy 0.556, and val rmse 1.355\n",
      "train loss 1.188, val loss 1.203, val accuracy 0.554, and val rmse 1.348\n",
      "train loss 1.112, val loss 1.165, val accuracy 0.574, and val rmse 1.189\n",
      "train loss 1.072, val loss 1.161, val accuracy 0.517, and val rmse 1.145\n",
      "train loss 1.042, val loss 1.117, val accuracy 0.563, and val rmse 1.279\n",
      "train loss 0.981, val loss 1.113, val accuracy 0.572, and val rmse 1.216\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.936, val loss 1.065, val accuracy 0.576, and val rmse 1.177\n",
      "train loss 0.846, val loss 1.009, val accuracy 0.603, and val rmse 0.937\n",
      "train loss 0.848, val loss 1.009, val accuracy 0.606, and val rmse 0.912\n",
      "train loss 0.784, val loss 0.994, val accuracy 0.604, and val rmse 0.894\n",
      "train loss 0.741, val loss 0.984, val accuracy 0.617, and val rmse 0.870\n",
      "train loss 0.702, val loss 0.999, val accuracy 0.623, and val rmse 0.863\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.695, val loss 1.010, val accuracy 0.619, and val rmse 0.869\n",
      "train loss 0.630, val loss 0.992, val accuracy 0.625, and val rmse 0.836\n",
      "train loss 0.583, val loss 1.020, val accuracy 0.632, and val rmse 0.823\n",
      "train loss 0.545, val loss 1.052, val accuracy 0.635, and val rmse 0.823\n",
      "train loss 0.502, val loss 1.088, val accuracy 0.634, and val rmse 0.827\n",
      "train loss 0.469, val loss 1.145, val accuracy 0.639, and val rmse 0.817\n"
     ]
    }
   ],
   "source": [
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with variable length input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_variable_input(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_variable_input(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.328, val loss 1.250, val accuracy 0.515, and val rmse 1.312\n",
      "train loss 1.031, val loss 1.063, val accuracy 0.577, and val rmse 1.017\n",
      "train loss 0.904, val loss 0.995, val accuracy 0.603, and val rmse 0.941\n",
      "train loss 0.849, val loss 1.000, val accuracy 0.599, and val rmse 0.940\n",
      "train loss 0.845, val loss 1.009, val accuracy 0.598, and val rmse 0.921\n",
      "train loss 0.834, val loss 1.005, val accuracy 0.593, and val rmse 0.902\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=30, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.828, val loss 1.000, val accuracy 0.599, and val rmse 0.920\n",
      "train loss 0.790, val loss 0.989, val accuracy 0.605, and val rmse 0.894\n",
      "train loss 0.775, val loss 0.992, val accuracy 0.614, and val rmse 0.884\n",
      "train loss 0.755, val loss 0.994, val accuracy 0.597, and val rmse 0.883\n",
      "train loss 0.738, val loss 0.987, val accuracy 0.608, and val rmse 0.872\n",
      "train loss 0.741, val loss 1.005, val accuracy 0.611, and val rmse 0.888\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=30, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.758, val loss 1.028, val accuracy 0.616, and val rmse 0.884\n",
      "train loss 0.725, val loss 0.994, val accuracy 0.621, and val rmse 0.877\n",
      "train loss 0.715, val loss 0.999, val accuracy 0.607, and val rmse 0.881\n",
      "train loss 0.707, val loss 1.008, val accuracy 0.608, and val rmse 0.879\n",
      "train loss 0.698, val loss 1.018, val accuracy 0.615, and val rmse 0.890\n",
      "train loss 0.686, val loss 1.017, val accuracy 0.603, and val rmse 0.893\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=30, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with pretrained Glove word embeddings\n",
    "\n",
    "Download weights from : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\"./data/glove.6B/glove.6B.50d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file) as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_glove_vecs(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.281, val loss 1.255, val accuracy 0.556, and val rmse 1.355\n",
      "train loss 1.210, val loss 1.207, val accuracy 0.556, and val rmse 1.354\n",
      "train loss 1.206, val loss 1.204, val accuracy 0.556, and val rmse 1.354\n",
      "train loss 1.201, val loss 1.202, val accuracy 0.556, and val rmse 1.354\n",
      "train loss 1.173, val loss 1.168, val accuracy 0.557, and val rmse 1.352\n",
      "train loss 1.131, val loss 1.122, val accuracy 0.562, and val rmse 1.249\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=30, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.112, val loss 1.113, val accuracy 0.556, and val rmse 1.349\n",
      "train loss 1.061, val loss 1.051, val accuracy 0.570, and val rmse 1.109\n",
      "train loss 1.014, val loss 1.014, val accuracy 0.582, and val rmse 1.058\n",
      "train loss 0.979, val loss 0.990, val accuracy 0.599, and val rmse 0.995\n",
      "train loss 0.948, val loss 0.961, val accuracy 0.610, and val rmse 0.950\n",
      "train loss 0.923, val loss 0.952, val accuracy 0.612, and val rmse 0.935\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=30, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.189, val loss 1.014, val accuracy 0.586, and val rmse 1.033\n",
      "train loss 0.946, val loss 0.964, val accuracy 0.606, and val rmse 0.950\n",
      "train loss 0.912, val loss 0.951, val accuracy 0.612, and val rmse 0.941\n",
      "train loss 0.895, val loss 0.949, val accuracy 0.615, and val rmse 0.913\n",
      "train loss 0.886, val loss 0.947, val accuracy 0.617, and val rmse 0.901\n",
      "train loss 0.872, val loss 0.938, val accuracy 0.621, and val rmse 0.890\n"
     ]
    }
   ],
   "source": [
    "train_model(model, epochs=30, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting ratings using regression instead of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_regr(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.float()\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss = validation_metrics_regr(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train mse %.3f val rmse %.3f\" % (sum_loss/total, val_loss))\n",
    "\n",
    "def validation_metrics_regr (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.float()\n",
    "        y_hat = model(x, l)\n",
    "        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_regr(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  LSTM_regr(vocab_size, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse 1.663 val rmse 1.313\n",
      "train mse 1.215 val rmse 1.125\n",
      "train mse 1.151 val rmse 1.109\n",
      "train mse 1.114 val rmse 1.115\n",
      "train mse 1.082 val rmse 1.121\n",
      "train mse 1.043 val rmse 1.116\n"
     ]
    }
   ],
   "source": [
    "train_model_regr(model, epochs=30, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse 1.214 val rmse 1.193\n",
      "train mse 0.884 val rmse 1.032\n",
      "train mse 0.631 val rmse 0.903\n",
      "train mse 0.483 val rmse 0.837\n",
      "train mse 0.416 val rmse 0.806\n",
      "train mse 0.363 val rmse 0.799\n"
     ]
    }
   ],
   "source": [
    "train_model_regr(model, epochs=30, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Saving notebook..\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit(\"lstm multiclass text classification, regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
